# Auto Code Reviewer

An intelligent code review agent that leverages tree-sitter for structural analysis, clangd for semantic understanding, and documentation context to provide comprehensive code reviews.

## ğŸ¯ Project Goals

Build a measurable, iterative code review system:
1. **Phase 1** (Current): MVP with tree-sitter + local MR testing
2. **Phase 2**: Add GitLab pattern analysis from historical reviews
3. **Phase 3**: Integrate clangd for semantic analysis
4. **Phase 4**: Add documentation context (Claude MDs, blogs, internal docs)
5. **Phase 5**: Experiment with doc quality improvements
6. **Phase 6**: CI/CD integration

## ğŸš€ Quick Start (Phase 1)

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd auto-code-reviewer

# Install dependencies
pip install -r requirements.txt --break-system-packages

# Install tree-sitter language grammars
python scripts/setup_tree_sitter.py
```

### Download Test MRs

```bash
# Set your GitLab credentials
export GITLAB_URL="https://gitlab.example.com"
export GITLAB_TOKEN="your_token_here"

# Download MRs for testing
python scripts/download_mrs.py --project-id 123 --output-dir test_data/mrs/
```

### Run Review on Local MR

```bash
# Review a single MR
python scripts/review_mr.py test_data/mrs/mr_456.json

# Run on all test MRs and generate quality report
python scripts/batch_review.py test_data/mrs/ --output-report quality_metrics.json
```

## ğŸ“ Project Structure

```
auto-code-reviewer/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ auto_reviewer/
â”‚       â”œâ”€â”€ parsers/
â”‚       â”‚   â”œâ”€â”€ mr_parser.py          # Parse GitLab MR JSON
â”‚       â”‚   â””â”€â”€ diff_parser.py        # Parse git diffs
â”‚       â”œâ”€â”€ analyzers/
â”‚       â”‚   â”œâ”€â”€ tree_sitter_analyzer.py  # Tree-sitter code analysis
â”‚       â”‚   â””â”€â”€ rules/                   # Review rule definitions
â”‚       â”‚       â”œâ”€â”€ base.py
â”‚       â”‚       â”œâ”€â”€ cpp_rules.py
â”‚       â”‚       â”œâ”€â”€ python_rules.py
â”‚       â”‚       â””â”€â”€ general_rules.py
â”‚       â”œâ”€â”€ reviewer.py               # Main reviewer orchestration
â”‚       â””â”€â”€ metrics.py                # Quality metrics tracking
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_tree_sitter.py         # Install language grammars
â”‚   â”œâ”€â”€ download_mrs.py              # Download MRs from GitLab
â”‚   â”œâ”€â”€ review_mr.py                 # Review single MR
â”‚   â””â”€â”€ batch_review.py              # Batch review with metrics
â”œâ”€â”€ test_data/
â”‚   â””â”€â”€ mrs/                         # Downloaded MR files
â”œâ”€â”€ config/
â”‚   â””â”€â”€ review_rules.yaml            # Configurable review rules
â””â”€â”€ tests/
    â””â”€â”€ test_analyzers.py
```

## ğŸ¯ Current Phase 1 Features

### Tree-sitter Analysis
- **Structural checks**: Function length, complexity, nesting depth
- **Pattern detection**: Missing returns, uninitialized variables, resource leaks
- **Style issues**: Naming conventions, formatting consistency
- **Language support**: C, C++, Python (extensible to others)

### Quality Metrics
- True positive rate (issues correctly identified)
- False positive rate (noise)
- Coverage (% of real issues caught)
- Review comment clarity
- Performance (time per file)

### MR Testing Framework
- Download real MRs from GitLab
- Review them locally
- Compare against human reviews
- Track improvements over time

## ğŸ”§ Configuration

Edit `config/review_rules.yaml` to customize:

```yaml
rules:
  general:
    max_function_length: 50
    max_nesting_depth: 4
    max_complexity: 15
  
  cpp:
    check_resource_leaks: true
    require_error_handling: true
    check_memory_safety: true
  
  python:
    check_exception_handling: true
    enforce_type_hints: false
```

## ğŸ“Š Quality Metrics

Track reviewer quality over time:

```bash
# Generate metrics report
python scripts/batch_review.py test_data/mrs/ --compare-with-human-reviews
```

Metrics tracked:
- Precision: What % of flagged issues are real?
- Recall: What % of real issues did we catch?
- Comment quality: How helpful are the suggestions?
- False positive rate by rule type

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Test specific analyzer
pytest tests/test_analyzers.py::TestTreeSitterAnalyzer

# Test with real MR
python scripts/review_mr.py test_data/mrs/mr_123.json --verbose
```

## ğŸ”œ Next Steps (Future Phases)

- **Phase 2**: GitLab pattern analysis from historical reviews
- **Phase 3**: Clangd integration (requires compile_commands.json)
- **Phase 4**: Documentation context retrieval
- **Phase 5**: LLM-powered review synthesis
- **Phase 6**: CI/CD integration

## ğŸ“ Contributing

See each phase's goals in the project documentation. Current focus: improving tree-sitter rule accuracy and reducing false positives.

## ğŸ“„ License

[Your License Here]
